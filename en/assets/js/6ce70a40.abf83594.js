"use strict";(self.webpackChunksummary=self.webpackChunksummary||[]).push([["8024"],{82099(e,n,t){t.r(n),t.d(n,{metadata:()=>i,default:()=>p,frontMatter:()=>r,contentTitle:()=>l,toc:()=>c,assets:()=>o});var i=JSON.parse('{"id":"ai-bigdata/AI/prompt-engineering-roadmap/level-4-production","title":"Level 4 - Full Stack Landing & Production Optimization","description":"Complete Frontend & Backend Full Stack Delivery, Master Model Fine-tuning, Establish Production-grade LLMOps System","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/ai-bigdata/AI/prompt-engineering-roadmap/level-4-production.mdx","sourceDirName":"ai-bigdata/AI/prompt-engineering-roadmap","slug":"/ai-bigdata/AI/prompt-engineering-roadmap/level-4-production","permalink":"/en/ai-bigdata/AI/prompt-engineering-roadmap/level-4-production","draft":false,"unlisted":false,"editUrl":"https://github.com/Halcyon666/summary/edit/main/docs/ai-bigdata/AI/prompt-engineering-roadmap/level-4-production.mdx","tags":[{"inline":true,"label":"ai","permalink":"/en/tags/ai"},{"inline":true,"label":"production","permalink":"/en/tags/production"},{"inline":true,"label":"fine-tuning","permalink":"/en/tags/fine-tuning"},{"inline":true,"label":"level-4","permalink":"/en/tags/level-4"}],"version":"current","lastUpdatedBy":"halcyon666","lastUpdatedAt":1769299200000,"sidebarPosition":5,"frontMatter":{"id":"level-4-production","title":"Level 4 - Full Stack Landing & Production Optimization","sidebar_label":"Level 4 - Full Stack Landing","sidebar_position":5,"description":"Complete Frontend & Backend Full Stack Delivery, Master Model Fine-tuning, Establish Production-grade LLMOps System","tags":["ai","production","fine-tuning","level-4"],"sources":["overview","level-3-agent"],"references":["summary","career"],"last_update":{"date":"2026-01-25","author":"halcyon666"}},"sidebar":"tutorialSidebar","previous":{"title":"Level 3 - Agent Architecture","permalink":"/en/ai-bigdata/AI/prompt-engineering-roadmap/level-3-agent"},"next":{"title":"Prompt Quality Evaluation & Summary","permalink":"/en/ai-bigdata/AI/prompt-engineering-roadmap/summary"}}'),s=t(62615),a=t(77545);let r={id:"level-4-production",title:"Level 4 - Full Stack Landing & Production Optimization",sidebar_label:"Level 4 - Full Stack Landing",sidebar_position:5,description:"Complete Frontend & Backend Full Stack Delivery, Master Model Fine-tuning, Establish Production-grade LLMOps System",tags:["ai","production","fine-tuning","level-4"],sources:["overview","level-3-agent"],references:["summary","career"],last_update:{date:"2026-01-25",author:"halcyon666"}},l="Phase 4: Full Stack Landing & Production Optimization (Level 4)",o={},c=[{value:"Prerequisite Capabilities",id:"prerequisite-capabilities",level:2},{value:"Why this phase is needed",id:"why-this-phase-is-needed",level:2},{value:"Core Capability 1: Frontend Application Delivery",id:"core-capability-1-frontend-application-delivery",level:2},{value:"Tool Selection Strategy",id:"tool-selection-strategy",level:3},{value:"React + FastAPI Full Stack Practice",id:"react--fastapi-full-stack-practice",level:3},{value:"Core Capability 2: Model Fine-tuning",id:"core-capability-2-model-fine-tuning",level:2},{value:"When Fine-tuning is Needed",id:"when-fine-tuning-is-needed",level:3},{value:"Tech Stack",id:"tech-stack",level:3},{value:"Practical Task: Fine-tune Qwen 2.5 to Write Java Code Comments",id:"practical-task-fine-tune-qwen-25-to-write-java-code-comments",level:3},{value:"\u2B50 Core Capability 3: Production Grade Deployment",id:"-core-capability-3-production-grade-deployment",level:2},{value:"\u2B50 Deployment Scheme Selection",id:"-deployment-scheme-selection",level:3},{value:"vLLM Deployment Example",id:"vllm-deployment-example",level:3},{value:"Core Capability 4: LLMOps Evaluation &amp; Ops",id:"core-capability-4-llmops-evaluation--ops",level:2},{value:"Perfect Evaluation System (Ragas / TruLens)",id:"perfect-evaluation-system-ragas--trulens",level:3},{value:"LangSmith Monitoring",id:"langsmith-monitoring",level:3},{value:"Phase Output Standards",id:"phase-output-standards",level:2},{value:"Roadmap Optimization Suggestions",id:"roadmap-optimization-suggestions",level:2}];function d(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"phase-4-full-stack-landing--production-optimization-level-4",children:"Phase 4: Full Stack Landing & Production Optimization (Level 4)"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cycle"}),": Weeks 9-12\n",(0,s.jsx)(n.strong,{children:"Core Goal"}),": Complete Frontend & Backend Full Stack Delivery, Master Model Fine-tuning, Establish Production-grade LLMOps System"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisite-capabilities",children:"Prerequisite Capabilities"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 Mastered Agent Architecture (Level 3)"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Established Observability and Audit Capability"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Implemented Complete Java-Python Communication"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"why-this-phase-is-needed",children:"Why this phase is needed"}),"\n",(0,s.jsx)(n.p,{children:'First three phases solved "AI Capability" problems, but enterprise applications also need "Engineering Capability": Frontend Delivery, Model Optimization, Production Deployment, Monitoring & Ops. This phase upgrades you from "Making Demo" to "Going to Production".'}),"\n",(0,s.jsx)(n.h2,{id:"core-capability-1-frontend-application-delivery",children:"Core Capability 1: Frontend Application Delivery"}),"\n",(0,s.jsx)(n.h3,{id:"tool-selection-strategy",children:"Tool Selection Strategy"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streamlit"}),": For internal AI debugging tools (Fast prototype, no need for complex UI)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"React"}),": For actual end-user products (Professional UI, Good UX)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"react--fastapi-full-stack-practice",children:"React + FastAPI Full Stack Practice"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"\u2B50 Frontend: Streaming Output + Markdown Rendering"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-typescript",children:"// React Component: Chat Interface\nimport { useState } from 'react';\nimport ReactMarkdown from 'react-markdown';\n\nfunction ChatInterface() {\n  const [messages, setMessages] = useState<Message[]>([]);\n  const [input, setInput] = useState('');\n  const [streaming, setStreaming] = useState(false);\n\n  const sendMessage = async () => {\n    setStreaming(true);\n    \n    // Call FastAPI Streaming Interface\n    const response = await fetch('/api/agent/stream', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ question: input })\n    });\n\n    const reader = response.body.getReader();\n    let currentMessage = '';\n\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n\n      const chunk = new TextDecoder().decode(value);\n      currentMessage += chunk;\n      \n      // Update UI in real-time\n      setMessages(prev => [...prev.slice(0, -1), { role: 'assistant', content: currentMessage }]);\n    }\n\n    setStreaming(false);\n  };\n\n  return (\n    <div className=\"chat-container\">\n      {messages.map((msg, idx) => (\n        <div key={idx} className={`message ${msg.role}`}>\n          <ReactMarkdown>{msg.content}</ReactMarkdown>\n        </div>\n      ))}\n      <input value={input} onChange={e => setInput(e.target.value)} />\n      <button onClick={sendMessage} disabled={streaming}>Send</button>\n    </div>\n  );\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Backend: FastAPI Streaming Output"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\n\napp = FastAPI()\n\n@app.post("/api/agent/stream")\nasync def stream_agent_response(request: AgentRequest):\n    async def generate():\n        # Run Agent, generate answer step by step\n        for chunk in agent_app.stream({"question": request.question}):\n            yield chunk["content"]\n    \n    return StreamingResponse(generate(), media_type="text/plain")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"core-capability-2-model-fine-tuning",children:"Core Capability 2: Model Fine-tuning"}),"\n",(0,s.jsx)(n.h3,{id:"when-fine-tuning-is-needed",children:"When Fine-tuning is Needed"}),"\n",(0,s.jsx)(n.p,{children:"When Prompt Engineering cannot satisfy requirements:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Require specific output format (e.g. Strict JSON Schema)"}),"\n",(0,s.jsx)(n.li,{children:"Require domain expertise (e.g. Medical, Legal terms)"}),"\n",(0,s.jsx)(n.li,{children:"Require specific writing style (e.g. Code comment style)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"tech-stack",children:"Tech Stack"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LoRA / QLoRA"}),": Parameter-Efficient Fine-Tuning, only train small amount of parameters"]}),"\n",(0,s.jsxs)(n.li,{children:["\u2B50 ",(0,s.jsx)(n.strong,{children:"LLaMA-Factory"}),": Recommended, provides Web UI, simplifies fine-tuning process"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"HuggingFace PEFT"}),": Code-level control, suitable for advanced users"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"practical-task-fine-tune-qwen-25-to-write-java-code-comments",children:"Practical Task: Fine-tune Qwen 2.5 to Write Java Code Comments"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"1. Prepare Training Data"})," (At least 100 samples):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "instruction": "Add Javadoc comments for the following Java method",\n    "input": "public User findById(Long id) { return userRepository.findById(id).orElse(null); }",\n    "output": "/**\\n * Query user info by user ID\\n * @param id User ID\\n * @return User object, return null if not exists\\n */\\npublic User findById(Long id) { return userRepository.findById(id).orElse(null); }"\n  }\n]\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"2. Fine-tune using LLaMA-Factory"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Start LLaMA-Factory Web UI\nllamafactory-cli webui\n\n# Or use command line\nllamafactory-cli train \\\n  --model_name_or_path Qwen/Qwen2.5-7B \\\n  --dataset java_comments \\\n  --finetuning_type lora \\\n  --output_dir ./output/qwen-java-comments \\\n  --num_train_epochs 3 \\\n  --per_device_train_batch_size 4\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"3. Evaluate Fine-tuning Effect"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Compare output quality before and after fine-tuning\nbase_model = load_model("Qwen/Qwen2.5-7B")\nfinetuned_model = load_model("./output/qwen-java-comments")\n\ntest_code = "public void saveUser(User user) { userRepository.save(user); }"\n\nprint("Base model:", base_model.generate(test_code))\nprint("Finetuned model:", finetuned_model.generate(test_code))\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-core-capability-3-production-grade-deployment",children:"\u2B50 Core Capability 3: Production Grade Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"-deployment-scheme-selection",children:"\u2B50 Deployment Scheme Selection"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Development Environment"}),": Ollama"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pros: Simple easy to use, suitable for local development"}),"\n",(0,s.jsx)(n.li,{children:"Cons: Weak concurrency (5-10 QPS)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u2B50 Production Environment"}),": vLLM"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pros: High throughput (100+ QPS), PagedAttention optimizes VRAM"}),"\n",(0,s.jsx)(n.li,{children:"Cons: Complex configuration, Requires GPU Server"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u2B50 Quantitative Comparison"})," (Performance metrics must master):"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Ollama"}),(0,s.jsx)(n.th,{children:"vLLM"}),(0,s.jsx)(n.th,{children:"Difference Explanation"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:["\u2B50 ",(0,s.jsx)(n.strong,{children:"QPS"})," (Queries Per Second)"]}),(0,s.jsx)(n.td,{children:"5-10"}),(0,s.jsx)(n.td,{children:"100+"}),(0,s.jsx)(n.td,{children:"vLLM concurrency capability is 10-20 times of Ollama"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"First Token Latency"})}),(0,s.jsx)(n.td,{children:"~500ms"}),(0,s.jsx)(n.td,{children:"~200ms"}),(0,s.jsx)(n.td,{children:"vLLM response speed 60% faster"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:["\u2B50 ",(0,s.jsx)(n.strong,{children:"Concurrency Support"})]}),(0,s.jsx)(n.td,{children:"Weak (Single Request Queue)"}),(0,s.jsx)(n.td,{children:"Strong (PagedAttention)"}),(0,s.jsx)(n.td,{children:"vLLM supports high concurrency scenarios"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"VRAM Optimization"})}),(0,s.jsx)(n.td,{children:"None"}),(0,s.jsx)(n.td,{children:"PagedAttention"}),(0,s.jsx)(n.td,{children:"vLLM VRAM utilization improved 30-40%"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Applicable Scenario"})}),(0,s.jsx)(n.td,{children:"Dev/Test"}),(0,s.jsx)(n.td,{children:"Production"}),(0,s.jsx)(n.td,{children:"Choose based on concurrency needs"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u2B50 Decision Criteria"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use Ollama"}),": Single user scenario, QPS < 10, e.g. Personal Assistant, Internal Tools"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Use vLLM"}),": Multi-user scenario, QPS > 50, e.g. Enterprise App, Public Service"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"vllm-deployment-example",children:"vLLM Deployment Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Install vLLM\npip install vllm\n\n# Start vLLM Service\npython -m vllm.entrypoints.openai.api_server \\\n  --model Qwen/Qwen2.5-7B \\\n  --tensor-parallel-size 2 \\\n  --max-num-seqs 256\n\n# Call vLLM API (Compatible with OpenAI format)\ncurl http://localhost:8000/v1/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "Qwen/Qwen2.5-7B",\n    "prompt": "Hello",\n    "max_tokens": 100\n  }\'\n'})}),"\n",(0,s.jsx)(n.h2,{id:"core-capability-4-llmops-evaluation--ops",children:"Core Capability 4: LLMOps Evaluation & Ops"}),"\n",(0,s.jsx)(n.h3,{id:"perfect-evaluation-system-ragas--trulens",children:"Perfect Evaluation System (Ragas / TruLens)"}),"\n",(0,s.jsx)(n.p,{children:"Based on Level 2, add more metrics:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ragas.metrics import (\n    faithfulness,           # Answer Faithfulness (Whether based on retrieved content)\n    answer_relevancy,       # Answer Relevancy (Whether answered probability)\n    context_precision,      # Context Precision (Whether retrieved content is relevant)\n    context_recall          # Context Recall (Whether retrieved all relevant content)\n)\n\nresults = evaluate(\n    golden_qa,\n    metrics=[faithfulness, answer_relevancy, context_precision, context_recall]\n)\n\n# Set Quality Threshold\nassert results['faithfulness'] > 0.8, \"Answer Faithfulness Insufficient\"\nassert results['answer_relevancy'] > 0.7, \"Answer Relevancy Insufficient\"\n"})}),"\n",(0,s.jsx)(n.h3,{id:"langsmith-monitoring",children:"LangSmith Monitoring"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from langsmith import Client\n\nclient = Client()\n\n# View Token Consumption and Cost\nruns = client.list_runs(project_name="my-rag-app")\ntotal_tokens = sum(run.total_tokens for run in runs)\ntotal_cost = sum(run.total_cost for run in runs)\n\nprint(f"Total tokens: {total_tokens}")\nprint(f"Total cost: ${total_cost:.2f}")\n\n# View Trace Link\nfor run in runs:\n    print(f"Run ID: {run.id}")\n    print(f"Latency: {run.latency_ms}ms")\n    print(f"Steps: {run.child_runs}")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"phase-output-standards",children:"Phase Output Standards"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deliverables Must Complete"})," (As final verification for Full Stack AI Engineer):"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Full Stack Application Layer"}),":"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Complete at least 1 Full Stack AI App (React + FastAPI + LangGraph), containing complete UI and backend service"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\u2B50 Implement streaming output and Markdown rendering, smooth UX"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Optimization Layer"}),":"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Complete at least 1 model fine-tuning experiment, prepare 100+ training samples, quantifiable improvement after fine-tuning (e.g. accuracy improved by 10% or output format compliance reached 95%)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Evaluation System Layer"}),":"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Establish complete evaluation system (Ragas/TruLens), containing at least 4 metrics (Faithfulness, Answer Relevancy, Context Precision, Context Recall)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ",(0,s.jsx)(n.strong,{children:"Quality Threshold"}),": Faithfulness > 0.8, Answer Relevancy > 0.7, Context Precision > 0.7"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Production Deployment Layer"}),":"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\u2B50 Understand production grade deployment schemes (vLLM/TGI), able to explain performance difference between Ollama vs vLLM (QPS, Latency, Concurrency Support)"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Integrate LangSmith monitoring, able to trace Token consumption and Trace link"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\u2B50 Establish monitoring and alert mechanism (Latency > 2s alert, cost over budget alert, error rate > 5% alert)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Capability Verification"}),":"]}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Able to independently design and implement a complete Full Stack AI App, from frontend to backend to model deployment"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","\u2B50 Able to choose appropriate deployment scheme (Ollama vs vLLM) based on business needs and provide data support"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Time Checkpoint"}),": If not completed after 3 weeks, suggest completing frontend delivery first, then gradually add fine-tuning and production deployment functions"]}),"\n",(0,s.jsx)(n.h2,{id:"roadmap-optimization-suggestions",children:"Roadmap Optimization Suggestions"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Practical Project"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.strong,{children:"Streamlit"})," to build internal AI debugging tool (Visualize RAG retrieval results, Agent decision process)"]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.strong,{children:"React"})," to build end-user facing product (Professional UI, Streaming Output, Markdown Rendering)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous Phase"}),": ",(0,s.jsx)(n.a,{href:"./level-3-agent",children:"Level 3 - Agent Architecture & Observability"})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Next Phase"}),": ",(0,s.jsx)(n.a,{href:"./summary",children:"Prompt Quality Evaluation & Summary"})]})]})}function p(e={}){let{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);