"use strict";(self.webpackChunksummary=self.webpackChunksummary||[]).push([["910"],{96378(e,a,i){i.r(a),i.d(a,{metadata:()=>n,default:()=>m,frontMatter:()=>r,contentTitle:()=>l,toc:()=>c,assets:()=>d});var n=JSON.parse('{"id":"ai-bigdata/BigData/hbase","title":"HBASE","description":"HBase is based on Hadoop storage. HDFS is write-once read-many, does not allow modification; HBase provides complete CRUD capabilities. How is \\"Update\\" implemented?","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/ai-bigdata/BigData/HBASE.mdx","sourceDirName":"ai-bigdata/BigData","slug":"/ai-bigdata/BigData/hbase","permalink":"/en/ai-bigdata/BigData/hbase","draft":false,"unlisted":false,"editUrl":"https://github.com/Halcyon666/summary/edit/main/docs/ai-bigdata/BigData/HBASE.mdx","tags":[{"inline":true,"label":"ai","permalink":"/en/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/en/tags/machine-learning"}],"version":"current","lastUpdatedBy":"halcyon666","lastUpdatedAt":1769299200000,"sidebarPosition":10,"frontMatter":{"id":"hbase","title":"HBASE","sidebar_label":"HBASE","sidebar_position":10,"description":"HBase is based on Hadoop storage. HDFS is write-once read-many, does not allow modification; HBase provides complete CRUD capabilities. How is \\"Update\\" implemented?","tags":["ai","machine-learning"],"sources":["hadoop"],"last_update":{"date":"2026-01-25","author":"halcyon666"}},"sidebar":"tutorialSidebar","previous":{"title":"Hadoop Overview","permalink":"/en/ai-bigdata/BigData/Hadoop/hadoop-overview"},"next":{"title":"Flink Docker Compose","permalink":"/en/ai-bigdata/BigData/flink-docker-compose"}}'),s=i(62615),t=i(77545),o=i(4040);let r={id:"hbase",title:"HBASE",sidebar_label:"HBASE",sidebar_position:10,description:'HBase is based on Hadoop storage. HDFS is write-once read-many, does not allow modification; HBase provides complete CRUD capabilities. How is "Update" implemented?',tags:["ai","machine-learning"],sources:["hadoop"],last_update:{date:"2026-01-25",author:"halcyon666"}},l="HBASE",d={},c=[{value:"Overview",id:"overview",level:2},{value:"HBase is a set of distributed, scalable, non-relational database based on Hadoop provided by Apache - Does not support SQL",id:"hbase-is-a-set-of-distributed-scalable-non-relational-database-based-on-hadoop-provided-by-apache---does-not-support-sql",level:3},{value:"Features",id:"features",level:2},{value:"HBase is suitable for storing sparse data - HBase can store structured and semi-structured data",id:"hbase-is-suitable-for-storing-sparse-data---hbase-can-store-structured-and-semi-structured-data",level:3},{value:"Row Key - Rowkey",id:"row-key---rowkey",level:3},{value:"Column Family - Column Family:",id:"column-family---column-family",level:3},{value:"Namespace - namespace",id:"namespace---namespace",level:3},{value:"Cell - Cell",id:"cell---cell",level:3},{value:"hbase.txt",id:"hbasetxt",level:3},{value:"Mechanisms",id:"mechanisms",level:2},{value:"1. Split table into parts from Row Key direction, each part is an HRegion. A table has at least 1 HRegion.",id:"1-split-table-into-parts-from-row-key-direction-each-part-is-an-hregion-a-table-has-at-least-1-hregion",level:3},{value:"1. When HBase starts, it registers an <code>/hbase</code> node on Zookeeper.",id:"1-when-hbase-starts-it-registers-an-hbase-node-on-zookeeper",level:3},{value:"HMaster",id:"hmaster",level:3},{value:"How to determine HRegion location for first read/write in HBase",id:"how-to-determine-hregion-location-for-first-readwrite-in-hbase",level:3},{value:"HRegionServer",id:"hregionserver",level:3},{value:"Compaction Mechanism",id:"compaction-mechanism",level:3},{value:"Write Process",id:"write-process",level:3},{value:"Read Process",id:"read-process",level:3}];function h(e){let a={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(a.header,{children:(0,s.jsx)(a.h1,{id:"hbase",children:"HBASE"})}),"\n",(0,s.jsx)(a.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(a.h3,{id:"hbase-is-a-set-of-distributed-scalable-non-relational-database-based-on-hadoop-provided-by-apache---does-not-support-sql",children:"HBase is a set of distributed, scalable, non-relational database based on Hadoop provided by Apache - Does not support SQL"}),"\n",(0,s.jsx)(a.h2,{id:"features",children:"Features"}),"\n",(0,s.jsx)(a.h3,{id:"hbase-is-suitable-for-storing-sparse-data---hbase-can-store-structured-and-semi-structured-data",children:"HBase is suitable for storing sparse data - HBase can store structured and semi-structured data"}),"\n",(0,s.jsxs)(a.p,{children:["In HBase, if you need to delete a table, you must first disable this table.\nWhen creating a table, if namespace is not specified, it defaults to ",(0,s.jsx)(a.code,{children:"default"}),'.\nHBase is based on Hadoop storage, essentially based on HDFS. HDFS features write-once read-many, not allowing modification; HBase provides complete CRUD capabilities. How is "Update" implemented?']}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsx)(a.li,{children:'HBase\'s "Update" is not modifying previous data but appending to the end of file. When adding every piece of data, a timestamp is added. When reading data, only returning the latest data achieves the effect of update.'}),"\n",(0,s.jsx)(a.li,{children:"Timestamp is called the version number of data - VERSION."}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"If not specified, by default only the last version of data is returned; if multiple versions are needed, it must be specified when creating the table to retain versions."}),"\n",(0,s.jsx)(a.h3,{id:"row-key---rowkey",children:"Row Key - Rowkey"}),"\n",(0,s.jsx)(a.p,{children:"a. Equivalent to Primary Key in Relational Database.\nb. Row Key does not need to be specified when creating table, but dynamically specified when adding data.\nc. Row Key is sorted lexicographically by default."}),"\n",(0,s.jsx)(a.h3,{id:"column-family---column-family",children:"Column Family - Column Family:"}),"\n",(0,s.jsx)(a.p,{children:"a. In HBase, we don't care about columns, we care about Column Families - When creating a table, need to specify column family but not columns. Columns can be dynamically added or deleted.\nb. A column family can contain 0 to multiple columns.\nc. A column family is equivalent to a table in Relational Database.\nd. Every table contains at least 1 column family."}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.code,{children:"create 'person','basic','expand'"})," // person: Row Key, basic, expand: Column Families"]}),"\n",(0,s.jsx)(a.h3,{id:"namespace---namespace",children:"Namespace - namespace"}),"\n",(0,s.jsx)(a.p,{children:"a. Equivalent to Database in Relational Database.\nb. If not specified, table is placed in default by default."}),"\n",(0,s.jsxs)(a.p,{children:[(0,s.jsx)(a.code,{children:"create 'hbasedemo:person','basic','expand'"})," // person table in hbasedemo namespace"]}),"\n",(0,s.jsx)(a.h3,{id:"cell---cell",children:"Cell - Cell"}),"\n",(0,s.jsx)(a.p,{children:"a. Row Key + Column Family + Column + Timestamp/Version uniquely locks a cell.\nb. Every cell contains a timestamp."}),"\n",(0,s.jsx)(a.h3,{id:"hbasetxt",children:"hbase.txt"}),"\n",(0,s.jsx)(a.h2,{id:"mechanisms",children:"Mechanisms"}),"\n",(0,s.jsx)(a.h3,{id:"1-split-table-into-parts-from-row-key-direction-each-part-is-an-hregion-a-table-has-at-least-1-hregion",children:"1. Split table into parts from Row Key direction, each part is an HRegion. A table has at least 1 HRegion."}),"\n",(0,s.jsxs)(a.ol,{start:"2",children:["\n",(0,s.jsx)(a.li,{children:"Each HRegion is assigned to a certain HRegionServer for management."}),"\n",(0,s.jsx)(a.li,{children:"Because Row Keys are ordered, data ranges between HRegions do not cross, so different Row Keys can go to different HRegionServers for processing."}),"\n",(0,s.jsx)(a.li,{children:"HRegion does not store data, data eventually lands on HDFS. HRegion manages data."}),"\n",(0,s.jsx)(a.li,{children:"When HRegion size (size of data managed by HRegion) reaches a certain limit (default 10G), HRegion will split into two equal HRegions. One of the split HRegions will be transferred to another HRegionServer for management. Note: Data transfer does not occur during this process."}),"\n",(0,s.jsx)(a.li,{children:"HRegionServer manages HRegion, HRegion manages data."}),"\n",(0,s.jsx)(a.li,{children:"HRegion is the minimum unit for HBase distributed storage and load balancing, but not minimum unit for data storage."}),"\n",(0,s.jsx)(a.li,{children:"Structure of HRegion:\na. Each HRegion contains 1 to multiple HStores, number of HStores is determined by number of Column Families.\nb. Each HStore contains 1 memStore (Write Cache) and 0 to multiple StoreFiles/HFiles.\nc. Data in HBase is eventually stored in HFiles, HFiles eventually land on HDFS."}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:""}),"\n",(0,s.jsxs)(a.h3,{id:"1-when-hbase-starts-it-registers-an-hbase-node-on-zookeeper",children:["1. When HBase starts, it registers an ",(0,s.jsx)(a.code,{children:"/hbase"})," node on Zookeeper."]}),"\n",(0,s.jsxs)(a.ol,{start:"2",children:["\n",(0,s.jsxs)(a.li,{children:["When Active HMaster starts, it registers a temporary child node ",(0,s.jsx)(a.code,{children:"/hbase/master"})," under ",(0,s.jsx)(a.code,{children:"/hbase"})," node of Zookeeper - Active HMaster maintains this temporary node via heartbeat. Meaning once HMaster is down, heartbeat is gone, this temporary node disappears, Zookeeper knows to find another one from Backup HMasters to switch to Active."]}),"\n",(0,s.jsxs)(a.li,{children:["When Backup HMaster starts, it registers temporary child node under ",(0,s.jsx)(a.code,{children:"/hbase/backup-masters"})," node of Zookeeper."]}),"\n",(0,s.jsxs)(a.li,{children:["When HRegionServer starts, it also registers child node under ",(0,s.jsx)(a.code,{children:"/hbase/rs"})," node of Zookeeper."]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"hmaster",children:"HMaster"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsxs)(a.li,{children:["In HBase, number of HMasters is not limited, can start arbitrary number of HMasters, via command: ",(0,s.jsx)(a.code,{children:"sh hbase-daemon.sh start master"}),"."]}),"\n",(0,s.jsx)(a.li,{children:"Whoever starts first is Active HMaster."}),"\n",(0,s.jsxs)(a.li,{children:["Active HMaster monitors node ",(0,s.jsx)(a.code,{children:"/hbase/backup-masters"})," on Zookeeper, monitors if number of child nodes under this node changes."]}),"\n",(0,s.jsxs)(a.li,{children:["Active HMaster monitors ",(0,s.jsx)(a.code,{children:"/hbase/backup-masters"})," every time it syncs messages."]}),"\n",(0,s.jsx)(a.li,{children:"In actual process, number of HMaster nodes generally does not exceed 3: 1 Active 2 Backup."}),"\n",(0,s.jsx)(a.li,{children:"Role/Responsibilities of HMaster:\na. Manage HRegionServer.\nb. Responsible for DDL (Table Structure Operations) of tables in HBase. DML (Data Operations) do not go through HMaster."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"how-to-determine-hregion-location-for-first-readwrite-in-hbase",children:"How to determine HRegion location for first read/write in HBase"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["a. Client caches position of ",(0,s.jsx)(a.code,{children:".meta."})," file after obtaining it.\nb. After reading ",(0,s.jsx)(a.code,{children:".meta."}),", client caches content of ",(0,s.jsx)(a.code,{children:".meta."})," file.\nc. If client crashes or HRegion transfer happens, cache becomes invalid, need to re-establish."]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"hregionserver",children:"HRegionServer"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"Role of HRegionServer is to manage HRegion. Official docs state: Each HRegionServer can manage 1000 HRegions, each HRegion can manage at most 10G data."}),"\n",(0,s.jsx)(a.li,{children:"HRegionServer contains 1 WAL, 1 BlockCache and 0 to multiple HRegions."}),"\n",(0,s.jsx)(a.li,{children:"WAL - Write Ahead Log -> HLog\na. WAL lands on HDFS.\nb. When HRegionServer receives write request, it first records request to WAL. After successful recording, it updates data to memStore. Purpose is to prevent data loss.\nc. After WAL reaches certain limit, a new WAL is generated. Meanwhile original WAL becomes oldWAL, which will be cleaned up regularly.\nd. Before HBase 0.94, WAL only allowed serial write; From HBase 0.94, NIO channel mechanism was introduced, allowing parallel write of WAL."}),"\n",(0,s.jsx)(a.li,{children:'BlockCache\na. BlockCache is essentially a read cache, maintained in memory.\nb. BlockCache follows "Locality" principle when caching - Guess:\ni. Temporal Locality: If a piece of data is read, probability of being read second time is higher than other data. HBase puts this data into read cache.\nii. Spatial Locality: If a piece of data is read, probability of adjacent data being read is higher than other data. So put adjacent data into cache too.\nc. Default size of BlockCache is 128M.\nd. BlockCache adopts LRU strategy.'}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"In actual process, if there are many scan operations, turn off BlockCache; if many get operations, consider using BlockCache."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"compaction-mechanism",children:"Compaction Mechanism"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"HBase provides 2 types of merging mechanisms:\na. minor compact: Merge several adjacent small HFiles of an HRegion into a large HFile. After merge, multiple HFiles still exist.\nb. major compact: Merge all HFiles of an HRegion into one HFile. After merge, only 1 HFile exists."}),"\n",(0,s.jsx)(a.li,{children:"Efficiency of minor compact is relatively higher, so HBase default is also minor compact."}),"\n"]}),"\n",(0,s.jsx)(a.p,{children:"During HFile merge process, data marked for deletion and obsolete data (multiple versions of data can be kept) will be discarded."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"write-process",children:"Write Process"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"When HBase receives write request (put/delete/deleteall), first determine which HRegion to write data to."}),"\n",(0,s.jsx)(a.li,{children:"Find corresponding HRegionServer, record write request to WAL, then update data to memStore."}),"\n",(0,s.jsx)(a.li,{children:"After data updated to memStore, it is sorted: Row Key lexicographical -> Column Family Name lexicographical -> Column Name lexicographical -> Timestamp reverse order."}),"\n",(0,s.jsx)(a.li,{children:"memStore maintained in memory, size is 128M."}),"\n",(0,s.jsx)(a.li,{children:"After memStore reaches certain condition, it flushes to generate a new StoreFile/HFile - Single HFile is ordered; If flushed multiple times, all HFiles are locally ordered among themselves."}),"\n",(0,s.jsx)(a.li,{children:"memStore flush conditions:\na. Automatically flush when memStore is full.\nb. Default when WAL reaches 1G, memStore also flushes, and generates a new WAL.\nc. When sum of memory occupied by all memStores on an HRegionServer reaches 35% of physical memory, flush the largest memStore."}),"\n",(0,s.jsx)(a.li,{children:"HFile eventually lands on HDFS."}),"\n",(0,s.jsxs)(a.li,{children:["Format of first version of HFile:\na. DataBlock: Stores data.\ni. DataBlock is minimum storage unit in HBase.\nii. DataBlock default is 64KB. Small DataBlock better for query (get); Large DataBlock better for traversal (scan).\niii. Read cache spatial locality caches by DataBlock unit.\niv. Each DataBlock consists of a Magic and multiple KeyValues.","\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"Magic: Random number, used for verification."}),"\n",(0,s.jsxs)(a.li,{children:["KeyValue: Actual data storage.\nb. MetaBlock: Stores metadata. Only appears in ",(0,s.jsx)(a.code,{children:".meta."})," file.\nc. FileInfo: File information. Description of current HFile.\nd. DataIndex: Records start byte of each DataBlock in file.\ne. MetaIndex: Records start byte of each MetaBlock in file.\nf. Trailer: At end of file, fixed 4 bytes. First 2 bytes record DataIndex position, last 2 bytes record MetaIndex position."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.li,{children:"In second version of HFile, a Bloom Filter was added."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(a.h3,{id:"read-process",children:"Read Process"}),"\n",(0,s.jsxs)(a.ul,{children:["\n",(0,s.jsxs)(a.li,{children:["\n",(0,s.jsxs)(a.ol,{children:["\n",(0,s.jsx)(a.li,{children:"When HBase receives read request, first lock unique HRegion. HRegion is managed by an HRegionServer, so effectively locked unique HRegionServer."}),"\n",(0,s.jsx)(a.li,{children:"First try to read data from BlockCache; if not found, try reading memStore; if not found in memStore, then verify reading HFile."}),"\n",(0,s.jsx)(a.li,{children:"When reading HFile, first filter out HFiles not in range based on Row Key range; after range filtering, use Bloom Filter for secondary filtering."}),"\n"]}),"\n"]}),"\n"]}),"\n","\n",(0,s.jsx)(o.A,{})]})}function m(e={}){let{wrapper:a}={...(0,t.R)(),...e.components};return a?(0,s.jsx)(a,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);