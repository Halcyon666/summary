"use strict";(self.webpackChunksummary=self.webpackChunksummary||[]).push([["9114"],{11871(e,n,i){i.r(n),i.d(n,{metadata:()=>s,default:()=>x,frontMatter:()=>r,contentTitle:()=>o,toc:()=>c,assets:()=>t});var s=JSON.parse('{"id":"ai-bigdata/BigData/Hadoop/full-hadoop-reference","title":"Hadoop","description":"Complete Hadoop guide covering HDFS, MapReduce, YARN, and cluster setup","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/ai-bigdata/BigData/Hadoop/full-hadoop-reference.mdx","sourceDirName":"ai-bigdata/BigData/Hadoop","slug":"/ai-bigdata/BigData/Hadoop/full-hadoop-reference","permalink":"/en/ai-bigdata/BigData/Hadoop/full-hadoop-reference","draft":false,"unlisted":false,"editUrl":"https://github.com/Halcyon666/summary/edit/main/docs/ai-bigdata/BigData/Hadoop/full-hadoop-reference.mdx","tags":[{"inline":true,"label":"ai","permalink":"/en/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/en/tags/machine-learning"}],"version":"current","lastUpdatedBy":"halcyon666","lastUpdatedAt":1769299200000,"sidebarPosition":10,"frontMatter":{"id":"full-hadoop-reference","title":"Hadoop","sidebar_label":"Hadoop","sidebar_position":10,"description":"Complete Hadoop guide covering HDFS, MapReduce, YARN, and cluster setup","tags":["ai","machine-learning"],"sources":["hadoop"],"last_update":{"date":"2026-01-25","author":"halcyon666"}},"sidebar":"tutorialSidebar","previous":{"title":"Hadoop","permalink":"/en/category/hadoop"},"next":{"title":"Hadoop Overview","permalink":"/en/ai-bigdata/BigData/Hadoop/hadoop-overview"}}'),l=i(62615),d=i(77545),a=i(4040);let r={id:"full-hadoop-reference",title:"Hadoop",sidebar_label:"Hadoop",sidebar_position:10,description:"Complete Hadoop guide covering HDFS, MapReduce, YARN, and cluster setup",tags:["ai","machine-learning"],sources:["hadoop"],last_update:{date:"2026-01-25",author:"halcyon666"}},o="Hadoop",t={},c=[{value:"Hadoop Versions",id:"hadoop-versions",level:2},{value:"Hadoop Modules",id:"hadoop-modules",level:2},{value:"Hadoop Installation",id:"hadoop-installation",level:2},{value:"Standalone and Pseudo-Distributed",id:"standalone-and-pseudo-distributed",level:3},{value:"Differences between Hadoop 2.0 and 3.0",id:"differences-between-hadoop-20-and-30",level:3},{value:"HDFS",id:"hdfs",level:2},{value:"NameNode",id:"namenode",level:3},{value:"DataNode",id:"datanode",level:3},{value:"SecondaryNameNode",id:"secondarynamenode",level:3},{value:"Replica Placement Strategy",id:"replica-placement-strategy",level:3},{value:"Rack Awareness",id:"rack-awareness",level:3},{value:"Trash Mechanism",id:"trash-mechanism",level:3},{value:"DFS Directory",id:"dfs-directory",level:3},{value:"Read/Write Process",id:"readwrite-process",level:3},{value:"MapReduce",id:"mapreduce",level:2},{value:"Overview",id:"overview",level:3},{value:"Maven Configuration",id:"maven-configuration",level:3},{value:"Data Localization Strategy",id:"data-localization-strategy",level:3},{value:"Shuffle",id:"shuffle",level:3},{value:"Data Skew",id:"data-skew",level:3},{value:"Small File Handling",id:"small-file-handling",level:3},{value:"YARN",id:"yarn",level:2},{value:"Overview",id:"overview-1",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Job Execution Flow",id:"job-execution-flow",level:3},{value:"Speculative Execution",id:"speculative-execution",level:3},{value:"Uber Reuse",id:"uber-reuse",level:3}];function h(e){let n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,d.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"hadoop",children:"Hadoop"})}),"\n",(0,l.jsx)(n.h2,{id:"hadoop-versions",children:"Hadoop Versions"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop 1.0"}),": Includes Common, HDFS, MapReduce"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop 2.0"}),": Includes Common, HDFS, MapReduce, YARN. Hadoop 1.0 and 2.0 are incompatible. Since Hadoop 2.7, includes Ozone. Since Hadoop 2.10, includes Submarine."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop 3.0"}),": Includes Common, HDFS, MapReduce, YARN, and Ozone module. Latest Hadoop 3.0 includes Submarine."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"hadoop-modules",children:"Hadoop Modules"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop Common"}),": Supports other modules"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"HDFS (Hadoop Distributed File System)"}),": Distributed storage"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop YARN"}),": Task scheduling and resource management"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop MapReduce"}),": Distributed computing framework based on YARN"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop Ozone"}),": Object storage"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop Submarine"}),": Machine Learning engine"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"hadoop-installation",children:"Hadoop Installation"}),"\n",(0,l.jsx)(n.h3,{id:"standalone-and-pseudo-distributed",children:"Standalone and Pseudo-Distributed"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Hadoop Pseudo-Distributed Installation.txt"}),"\n",(0,l.jsx)(n.li,{children:"Hadoop Fully Distributed Setup.txt"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"differences-between-hadoop-20-and-30",children:"Differences between Hadoop 2.0 and 3.0"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://www.jianshu.com/p/fb5fa03a2f03",children:"Reference Link"})}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"hdfs",children:"HDFS"}),"\n",(0,l.jsx)(n.h3,{id:"namenode",children:"NameNode"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Role"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Manage DataNodes and store metadata."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Metadata"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"a. File storage path"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"b. File permissions"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"c. File size"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"d. Block size"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"e. Mapping between File and BlockID"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"f. Mapping between BlockID and DataNode"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"g. Replication factor"}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"Metadata is irrelevant to specific content of files."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"Metadata size is 130~180B."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"a. Maintained in memory for fast R/W."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"b. Maintained on disk for crash recovery."}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Storage files on disk"}),":"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"edits"}),": Operation file. Records write operations."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"fsimage"}),": Meta image file. Records metadata. This file's metadata often lags behind metadata in memory."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["Metadata storage path determined by ",(0,l.jsx)(n.code,{children:"hadoop.tmp.dir"})," property. Defaults to ",(0,l.jsx)(n.code,{children:"/tmp"})," if unspecified."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["When NameNode receives write request, it first writes request to ",(0,l.jsx)(n.code,{children:"edits_inprogress"})," file. If successful, updates metadata in memory. If memory update successful, returns ack to client. ",(0,l.jsx)(n.code,{children:"fsimage"})," is not modified in this process."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:["As HDFS runs, ",(0,l.jsx)(n.code,{children:"edits_inprogress"})," grows. Gap between ",(0,l.jsx)(n.code,{children:"fsimage"})," and memory metadata grows. Need to update ",(0,l.jsx)(n.code,{children:"fsimage"})," at appropriate time. ",(0,l.jsx)(n.code,{children:"edits_inprogress"})," will roll, producing an ",(0,l.jsx)(n.code,{children:"edits"})," file and a new ",(0,l.jsx)(n.code,{children:"edits_inprogress"}),"."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Space"}),": When ",(0,l.jsx)(n.code,{children:"edits"})," file reaches specified size (default 64M, ",(0,l.jsx)(n.code,{children:"fs.checkpoint.size"}),"), rolls new ",(0,l.jsx)(n.code,{children:"edits_inprogress"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Time"}),": When time interval from last roll reaches condition (default 1H, ",(0,l.jsx)(n.code,{children:"fs.checkpoint.period"}),"), rolls new ",(0,l.jsx)(n.code,{children:"edits_inprogress"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Force"}),": ",(0,l.jsx)(n.code,{children:"hadoop dfsadmin -rollEdits"})]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Restart"}),": NameNode restart triggers ",(0,l.jsx)(n.code,{children:"edits"})," file rolling."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsx)(n.p,{children:"NameNode manages DataNode via Heartbeat."}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Default heartbeat interval 3s (",(0,l.jsx)(n.code,{children:"dfs.heartbeat.interval"}),")."]}),"\n",(0,l.jsx)(n.li,{children:"If NameNode receives no heartbeat from DataNode for over 10min, considers node lost. Data on this node is backed up to other nodes to ensure replication factor."}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Heartbeat Signal"}),":","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Current DataNode status (Pre-service, In-service, Pre-decommission, Decommission)"}),"\n",(0,l.jsx)(n.li,{children:"BlockIDs in DataNode"}),"\n",(0,l.jsx)(n.li,{children:"clusterID. Generated when NameNode formatted. Sent to DataNode on startup. DataNode carries clusterID in heartbeat for validation."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"NameNode/HDFS Restart"}),": Triggers ",(0,l.jsx)(n.code,{children:"edits_inprogress"})," rolling, updates operations to ",(0,l.jsx)(n.code,{children:"fsimage"}),", loads ",(0,l.jsx)(n.code,{children:"fsimage"})," to memory, waits for DataNode heartbeat, verifies blocks. This process is ",(0,l.jsx)(n.strong,{children:"Safe Mode"}),". Automatically exits upon success."]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Force exit safe mode: ",(0,l.jsx)(n.code,{children:"hadoop dfsadmin -safemode leave"})]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"datanode",children:"DataNode"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Block"}),": Data split into specified size blocks.","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Meaning: 1) Store huge files 2) Fast backup"}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Role"}),": Store Block on DataNode disk."]}),"\n",(0,l.jsx)(n.li,{children:"Sends heartbeat to NameNode periodically."}),"\n",(0,l.jsxs)(n.li,{children:["Storage location determined by ",(0,l.jsx)(n.code,{children:"hadoop.tmp.dir"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["Default 128M (",(0,l.jsx)(n.code,{children:"dfs.blocksize"}),")."]}),"\n",(0,l.jsx)(n.li,{children:"Smaller files use actual size."}),"\n",(0,l.jsx)(n.li,{children:"Global incremental Block ID."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"secondarynamenode",children:"SecondaryNameNode"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Not hot backup of NameNode. Assists NameNode in ",(0,l.jsx)(n.code,{children:"edits"})," file rolling."]}),"\n",(0,l.jsx)(n.li,{children:"If SecondaryNameNode exists, it handles rolling. If not, NameNode handles it."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"replica-placement-strategy",children:"Replica Placement Strategy"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"1st Replica"}),":","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Internal upload: On uploading DataNode."}),"\n",(0,l.jsx)(n.li,{children:"External upload: NameNode chooses relatively idle DataNode."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"2nd Replica"}),":","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Pre-Hadoop 2.7: Different rack from 1st."}),"\n",(0,l.jsx)(n.li,{children:"Post-Hadoop 2.7: Same rack as 1st."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"3rd Replica"}),":","\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Pre-Hadoop 2.7: Same rack as 2nd."}),"\n",(0,l.jsx)(n.li,{children:"Post-Hadoop 2.7: Different rack from 2nd."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"More Replicas"}),": On relatively idle nodes."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"rack-awareness",children:"Rack Awareness"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Mapping IP/Hostname to a logical rack ID."}),"\n",(0,l.jsx)(n.li,{children:"Configured via script (Shell/Python)."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"trash-mechanism",children:"Trash Mechanism"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Default disabled."}),"\n",(0,l.jsxs)(n.li,{children:["Config in ",(0,l.jsx)(n.code,{children:"core-site.xml"}),": ",(0,l.jsx)(n.code,{children:"fs.trash.interval"})," (minutes). Default 0 (disabled). 1440 (1 day)."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"dfs-directory",children:"DFS Directory"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Controlled by ",(0,l.jsx)(n.code,{children:"hadoop.tmp.dir"}),". Created on format."]}),"\n",(0,l.jsxs)(n.li,{children:["Subdirectories: ",(0,l.jsx)(n.code,{children:"data"})," (DataNode), ",(0,l.jsx)(n.code,{children:"name"})," (NameNode), ",(0,l.jsx)(n.code,{children:"namesecondary"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"in_use.lock"}),": Marks process running."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"readwrite-process",children:"Read/Write Process"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Read"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Client -> NameNode (Check path)."}),"\n",(0,l.jsx)(n.li,{children:"NameNode -> Client (Block locations)."}),"\n",(0,l.jsx)(n.li,{children:"Client -> DataNode (Read Block, verify checksum)."}),"\n",(0,l.jsx)(n.li,{children:"Repeat for subsequent blocks."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Write"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Client -> NameNode (Check permission/existence)."}),"\n",(0,l.jsx)(n.li,{children:"NameNode -> Client (Allow)."}),"\n",(0,l.jsx)(n.li,{children:"Client -> NameNode (Request 1st Block location)."}),"\n",(0,l.jsx)(n.li,{children:"NameNode -> Client (DataNode list)."}),"\n",(0,l.jsx)(n.li,{children:"Client -> Pipeline write (DataNode1 -> DataNode2 -> DataNode3). Acks propagate back."}),"\n",(0,l.jsx)(n.li,{children:"Repeat for subsequent blocks."}),"\n",(0,l.jsx)(n.li,{children:"Client -> NameNode (Close file)."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Delete"}),":"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Client -> NameNode."}),"\n",(0,l.jsx)(n.li,{children:"NameNode marks deletion in metadata. Data remains on DataNode."}),"\n",(0,l.jsx)(n.li,{children:"DataNode deletes block upon Heartbeat response from NameNode."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"mapreduce",children:"MapReduce"}),"\n",(0,l.jsx)(n.h3,{id:"overview",children:"Overview"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Distributed computing framework provided by Hadoop."}),"\n",(0,l.jsx)(n.li,{children:"Based on Google MapReduce."}),"\n",(0,l.jsx)(n.li,{children:"Split into Map phase and Reduce phase."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"maven-configuration",children:"Maven Configuration"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-xml",children:"<plugin>\n    <groupId>org.apache.maven.plugins</groupId>\n    <artifactId>maven-jar-plugin</artifactId>\n    <configuration>\n        <archive>\n            <manifest>\n                <addClasspath>true</addClasspath>\n                <mainClass>cn.App</mainClass>\n            </manifest>\n        </archive>\n    </configuration>\n</plugin>\n"})}),"\n",(0,l.jsx)(n.h3,{id:"data-localization-strategy",children:"Data Localization Strategy"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"JobTracker accesses NameNode for metadata -> Slicing (Logical split)."}),"\n",(0,l.jsx)(n.li,{children:"Assign map tasks to nodes holding data (reduce transmission)."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"shuffle",children:"Shuffle"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Map Side"}),":","\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.code,{children:"context.write"})," writes to buffer (Ring buffer, default 100M)."]}),"\n",(0,l.jsx)(n.li,{children:"Partition, Sort in buffer (Quicksort)."}),"\n",(0,l.jsx)(n.li,{children:"Spill to disk when threshold reached (0.8). Generates spill files (Locally ordered)."}),"\n",(0,l.jsx)(n.li,{children:"Merge spill files to final output (Mergesort). Combiner if applicable."}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Reduce Side"}),":","\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"ReduceTask starts fetch thread to pull data from MapTask."}),"\n",(0,l.jsx)(n.li,{children:"Merge pulled files (Mergesort)."}),"\n",(0,l.jsx)(n.li,{children:"Grouping (Group values by key)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"data-skew",children:"Data Skew"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Map Side"}),": Rare. Caused by non-splittable uneven files."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Reduce Side"}),": Common. Caused by key distribution."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Solution"}),": Two-stage aggregation (Scatter then Aggregate)."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"small-file-handling",children:"Small File Handling"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Problems"}),": High memory usage on NameNode, High thread overhead on MapReduce."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Hadoop Archive"}),": ",(0,l.jsx)(n.code,{children:"hadoop archive -archiveName txt.har -p /result/ /"}),"."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"yarn",children:"YARN"}),"\n",(0,l.jsx)(n.h3,{id:"overview-1",children:"Overview"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Yet Another Resource Negotiator. Introduced in Hadoop 2.0."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"architecture",children:"Architecture"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"ResourceManager"}),": Master. Resource management."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"NodeManager"}),": Slave. Task monitoring and computing."]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"job-execution-flow",children:"Job Execution Flow"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Client submits job to ",(0,l.jsx)(n.strong,{children:"ApplicationsManager"}),"."]}),"\n",(0,l.jsxs)(n.li,{children:["ApplicationsManager assigns to NodeManager, requesting start of ",(0,l.jsx)(n.strong,{children:"ApplicationMaster"}),"."]}),"\n",(0,l.jsx)(n.li,{children:"ApplicationMaster starts on NodeManager."}),"\n",(0,l.jsx)(n.li,{children:"ApplicationMaster splits job to MapTask/ReduceTask."}),"\n",(0,l.jsx)(n.li,{children:"ApplicationMaster requests resources from ResourceManager."}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"ResourceScheduler"})," returns Container to ApplicationMaster."]}),"\n",(0,l.jsx)(n.li,{children:"ApplicationMaster assigns tasks to NodeManagers."}),"\n",(0,l.jsx)(n.li,{children:"NodeManager executes tasks."}),"\n",(0,l.jsx)(n.li,{children:"ApplicationMaster monitors execution."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"speculative-execution",children:"Speculative Execution"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Optimization for slow tasks."}),"\n",(0,l.jsx)(n.li,{children:"Run backup task on another node. First data wins."}),"\n",(0,l.jsx)(n.li,{children:"Not suitable for data skew scenarios (wastes resources)."}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"uber-reuse",children:"Uber Reuse"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:"Run small application tasks within same JVM of ApplicationMaster."}),"\n",(0,l.jsxs)(n.li,{children:["Config: ",(0,l.jsx)(n.code,{children:"mapreduce.job.ubertask.enable"}),"."]}),"\n"]}),"\n","\n",(0,l.jsx)(a.A,{})]})}function x(e={}){let{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(h,{...e})}):h(e)}}}]);