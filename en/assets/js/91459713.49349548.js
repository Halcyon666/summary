"use strict";(self.webpackChunksummary=self.webpackChunksummary||[]).push([["9393"],{51508(e,n,t){t.r(n),t.d(n,{metadata:()=>s,default:()=>p,frontMatter:()=>i,contentTitle:()=>o,toc:()=>c,assets:()=>l});var s=JSON.parse('{"id":"ai-bigdata/AI/mcp","title":"MCP","description":"Detailed Explanation of MCP Protocol","source":"@site/i18n/en/docusaurus-plugin-content-docs/current/ai-bigdata/AI/mcp.mdx","sourceDirName":"ai-bigdata/AI","slug":"/ai-bigdata/AI/mcp","permalink":"/en/ai-bigdata/AI/mcp","draft":false,"unlisted":false,"editUrl":"https://github.com/Halcyon666/summary/edit/main/docs/ai-bigdata/AI/mcp.mdx","tags":[{"inline":true,"label":"ai","permalink":"/en/tags/ai"},{"inline":true,"label":"machine-learning","permalink":"/en/tags/machine-learning"}],"version":"current","lastUpdatedBy":"halcyon666","lastUpdatedAt":1769299200000,"sidebarPosition":5,"frontMatter":{"id":"mcp","title":"MCP","sidebar_label":"MCP","sidebar_position":5,"description":"Detailed Explanation of MCP Protocol","tags":["ai","machine-learning"],"sources":["level-1-ai-native"],"references":[],"last_update":{"date":"2026-01-25","author":"halcyon666"}},"sidebar":"tutorialSidebar","previous":{"title":"Prompt Quality Evaluation & Summary","permalink":"/en/ai-bigdata/AI/prompt-engineering-roadmap/summary"},"next":{"title":"AI Problems","permalink":"/en/ai-bigdata/AI/ai-problems"}}'),a=t(62615),r=t(77545);let i={id:"mcp",title:"MCP",sidebar_label:"MCP",sidebar_position:5,description:"Detailed Explanation of MCP Protocol",tags:["ai","machine-learning"],sources:["level-1-ai-native"],references:[],last_update:{date:"2026-01-25",author:"halcyon666"}},o="MCP",l={},c=[{value:"What is MCP?",id:"what-is-mcp",level:2},{value:"Core Components of MCP",id:"core-components-of-mcp",level:2},{value:"Working Principle of MCP",id:"working-principle-of-mcp",level:2},{value:"Advantages of MCP",id:"advantages-of-mcp",level:2},{value:"Application Scenarios of MCP",id:"application-scenarios-of-mcp",level:2},{value:"Practice Examples",id:"practice-examples",level:2},{value:"Simple Implementation of MCP Server",id:"simple-implementation-of-mcp-server",level:3},{value:"Using Claude Desktop App to Call MCP Server",id:"using-claude-desktop-app-to-call-mcp-server",level:3},{value:"Using MCP Inspector to Call MCP Server",id:"using-mcp-inspector-to-call-mcp-server",level:3},{value:"Using LangChain to Call MCP Server",id:"using-langchain-to-call-mcp-server",level:3}];function d(e){let n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"mcp",children:"MCP"})}),"\n",(0,a.jsx)(n.h2,{id:"what-is-mcp",children:"What is MCP?"}),"\n",(0,a.jsx)(n.p,{children:"MCP (Model Context Protocol) is a standardized protocol designed to simplify interaction between Large Language Models (LLM) and external data sources (such as databases, APIs, etc.).\nThrough MCP, developers can let LLM access and operate various data sources in a unified way, thereby improving utility and scalability of models."}),"\n",(0,a.jsx)(n.h2,{id:"core-components-of-mcp",children:"Core Components of MCP"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MCP Server"}),": Responsible for processing requests from LLM and forwarding them to corresponding data source. It acts as intermediary between LLM and external systems."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"MCP Client"}),": Integrated in LLM, used for sending requests to MCP Server and receiving responses."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Source Adapter"}),": Used for connecting different types of data sources (such as SQL databases, NoSQL databases, RESTful APIs, etc.) and converting data into format understandable by MCP."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"working-principle-of-mcp",children:"Working Principle of MCP"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Request Sending"}),": LLM sends request to MCP Server via MCP Client, specifying required data operation (such as query, insert, update, etc.)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Request Processing"}),": After receiving request, MCP Server parses request content and calls corresponding data source adapter."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Operation"}),": Data source adapter interacts with external data source and executes required operation."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Response Return"}),": After operation is completed, data source adapter returns result to MCP Server, which then sends result back to LLM."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"advantages-of-mcp",children:"Advantages of MCP"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Standardized Interface"}),": Through unified protocol, simplifies process of integrating LLM with multiple data sources."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Strong Scalability"}),": Supports multiple data source types, facilitating future extension and integration of new data sources."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Improve Efficiency"}),": Reduces repetitive work for developers when integrating data sources, improving development efficiency."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enhance Model Capabilities"}),": Enables LLM to access more real-time and structured data, improving accuracy and relevance of its answers."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"application-scenarios-of-mcp",children:"Application Scenarios of MCP"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Enterprise Knowledge Base"}),": Let LLM access enterprise internal databases to provide more accurate business consultation and support."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Dynamic Data Query"}),": Connect real-time data sources via MCP, such as stock market data,\nweather information, etc., enhancing real-time response capability of model."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-Source Integration"}),": Integrate multiple data sources in one application, such as CRM system, ERP system, etc., providing unified query interface."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Customized Application"}),": Customize development of MCP adapters according to specific business needs to achieve specific data interaction functions."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"practice-examples",children:"Practice Examples"}),"\n",(0,a.jsx)(n.h3,{id:"simple-implementation-of-mcp-server",children:"Simple Implementation of MCP Server"}),"\n",(0,a.jsx)(n.p,{children:"Here is a simple MCP Server implementation example using FastAPI framework:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP("Simple-Math-Server")\n\n\n@mcp.tool()\ndef add_numbers(a: int, b: int) -> dict:\n    """Add two numbers together and return a structured object."""\n    total = a + b\n\n    # Return a dictionary that matches your desired "properties" structure\n    return {\n        "result": total\n    }\n\n\nif __name__ == "__main__":\n    mcp.run()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"using-claude-desktop-app-to-call-mcp-server",children:"Using Claude Desktop App to Call MCP Server"}),"\n",(0,a.jsx)(n.p,{children:"Claude MCP Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "mcpServers": {\n    "my-math-tool": {\n      "command": "python",\n      "args": ["D:/project/PycharmProjects/fastApiProject/mcp-server.py"]\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"File Location:"}),"\n",(0,a.jsxs)(n.p,{children:["Mac: ",(0,a.jsx)(n.code,{children:"~/Library/Application Support/Claude/claude_desktop_config.json"})]}),"\n",(0,a.jsxs)(n.p,{children:["Windows: ",(0,a.jsx)(n.code,{children:"%APPDATA%\\Claude\\claude_desktop_config.json"})]}),"\n",(0,a.jsx)(n.p,{children:"Omitted..."}),"\n",(0,a.jsx)(n.h3,{id:"using-mcp-inspector-to-call-mcp-server",children:"Using MCP Inspector to Call MCP Server"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Start MCP Server:"}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-command",children:"npx @modelcontextprotocol/inspector python mcp-server.py \n"})}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"pip install mcp"})," should be installed in global environment, not just in virtual environment, otherwise Claude cannot find MCP package."]})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:"Connect in web page"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://s2.loli.net/2026/01/18/wzAijy7YTvnCkoR.png",alt:"20260118205955"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsxs)(n.li,{children:["Call ",(0,a.jsx)(n.code,{children:"add_numbers"})," method"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://s2.loli.net/2026/01/18/xqip2LnU1B9aFgS.png",alt:"20260118210144"})}),"\n",(0,a.jsx)(n.h3,{id:"using-langchain-to-call-mcp-server",children:"Using LangChain to Call MCP Server"}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"mcp-client.py"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport sys\nimport operator\nfrom typing import Annotated, TypedDict, List\n\n# MCP SDK\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# LangChain / LangGraph\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.tools import StructuredTool\nfrom langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\nfrom langgraph.graph import StateGraph, END\n\n# --- 1. Define State ---\nclass AgentState(TypedDict):\n    messages: Annotated[List[BaseMessage], operator.add]\n\n# --- 2. Converter: Turn MCP Tool into LangChain Tool ---\ndef mcp_to_langchain(mcp_tool, session):\n    async def _tool_func(**kwargs):\n        # Call MCP Server\n        result = await session.call_tool(mcp_tool.name, arguments=kwargs)\n        # Extract result text\n        if result.content and result.content[0].type == "text":\n            return result.content[0].text\n        return str(result)\n\n    return StructuredTool.from_function(\n        func=None,\n        coroutine=_tool_func, # LangChain supports async tools\n        name=mcp_tool.name,\n        description=mcp_tool.description\n    )\n\n# --- 3. Main Program ---\nasync def main():\n    # Configure Server Start Parameters (Ensure server.py and client.py are in same directory)\n    server_params = StdioServerParameters(\n        command=sys.executable, \n        args=["D:/project/PycharmProjects/fastApiProject/mcp-server.py"], \n        env=None\n    )\n\n    print("\u{1F50C} Client: Connecting to MCP Server...")\n\n    # Establish Connection\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            # Initialize\n            await session.initialize()\n            \n            # Get Tools\n            tools_data = await session.list_tools()\n            mcp_tools = tools_data.tools\n            print(f"\u{1F6E0}\uFE0F  Client: Found Tools -> {[t.name for t in mcp_tools]}")\n\n            # Convert Tools to LangChain\n            lc_tools = [mcp_to_langchain(t, session) for t in mcp_tools]\n\n            # Initialize LLM (Ensure your Ollama has pulled llama3.1 or qwen2.5)\n            llm = ChatOllama(model="llama3.1", temperature=0)\n            llm_with_tools = llm.bind_tools(lc_tools)\n\n            # --- Build LangGraph ---\n\n            async def call_model(state: AgentState):\n                # Call LLM\n                response = await llm_with_tools.ainvoke(state["messages"])\n                return {"messages": [response]}\n\n            async def call_tools(state: AgentState):\n                last_message = state["messages"][-1]\n                results = []\n                for call in last_message.tool_calls:\n                    print(f"\u{1F916} Agent: Decided to call tool \'{call[\'name\']}\' Args: {call[\'args\']}")\n                    \n                    # Find and Execute Tool\n                    tool = next((t for t in lc_tools if t.name == call[\'name\']), None)\n                    if tool:\n                        output = await tool.coroutine(**call[\'args\'])\n                        print(f"\u2705 Agent: Tool returned result -> {output}")\n                        \n                        results.append(ToolMessage(\n                            content=output,\n                            tool_call_id=call["id"],\n                            name=call["name"]\n                        ))\n                return {"messages": results}\n\n            # Define Graph Structure\n            workflow = StateGraph(AgentState)\n            workflow.add_node("llm", call_model)\n            workflow.add_node("tools", call_tools)\n            workflow.set_entry_point("llm")\n\n            # Conditional Edge: If tool_calls exist go to tools, otherwise end\n            workflow.add_conditional_edges(\n                "llm",\n                lambda s: "tools" if s["messages"][-1].tool_calls else END\n            )\n            workflow.add_edge("tools", "llm")\n\n            agent = workflow.compile()\n\n            # --- Run Test ---\n            query = "Please calculate how much is 100 plus 55?"\n            print(f"\\n\u{1F464} User: {query}")\n            print("-" * 50)\n\n            inputs = {"messages": [HumanMessage(content=query)]}\n            \n            # Run Graph\n            async for chunk in agent.astream(inputs, stream_mode="values"):\n                # Only print content of last message of every step\n                msg = chunk["messages"][-1]\n                # print(f"[{msg.type}]: {msg.content}") \n\n            # Print Final Reply\n            print("-" * 50)\n            print(f"\u{1F4A1} Final Answer: {chunk[\'messages\'][-1].content}")\n\nif __name__ == "__main__":\n    asyncio.run(main())\n    \n'})})]}),"\n",(0,a.jsx)(n.p,{children:"Output:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-log",children:"\n\u{1F50C} Client: Connecting to MCP Server...\n\u{1F6E0}\uFE0F  Client: Found Tools -> ['add_numbers']\n\n\u{1F464} User: Please calculate how much is 100 plus 55?\n--------------------------------------------------\n\u{1F916} Agent: Decided to call tool 'add_numbers' Args: {'a': 100, 'b': 55}\n\u2705 Agent: Tool returned result -> {\n  \"result\": 155\n}\n--------------------------------------------------\n\u{1F4A1} Final Answer: 100 plus 55 equals 155.\n    \n"})})]})}function p(e={}){let{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);